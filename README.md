<h2>The Project contains code implementation of probabilistic retrieval methods from the scratch</h1>

A comprehensive explanation of each of the tasks related to the .py files uploaded can be found in the report present in the directory. 

All the code has been run on the given dataset and uses dependencies such as NLTK and SPACY to do perform same basic pre-processing. 

<h2> Information on the tasks </h2>

The tasks can all be loaded as standalone .py files and run as scripts and assume the data to be present in the same directory as the .py file. 
The dependencies can be looked at from libraries each task imports. 

<h3> Task 1 </h3>

The first task is based on verifying the Zipf's law on the given dataset. 

<h3> Task 2 </h3> 

Task 2 implements an inverted index on the given dataset using no other dependencies other than the libraries used for pre-processing. 

<h3> Task 3 </h3> 

Task 3 implements a calculates the TF-IDF, then moves on to calculate scores using Cosine Similarity and a BM25 model all implemented from the scratch. 

<h3> Task 3 </h3> 

Task 4 implements Query Likelihood Models the details of which can be found in the attached reports. 

<h2> Dataset available on request, write to me at tabish390@gmail.com </h2>
